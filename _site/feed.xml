<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.10.0">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2024-12-07T01:26:42-05:00</updated><id>http://localhost:4000/feed.xml</id><title type="html">Working Title Blog</title><subtitle>It&apos;s a blog about things.</subtitle><entry><title type="html">Status Update #4: 12/7</title><link href="http://localhost:4000/2024/12/07/daily-4.html" rel="alternate" type="text/html" title="Status Update #4: 12/7" /><published>2024-12-07T01:21:02-05:00</published><updated>2024-12-07T01:21:02-05:00</updated><id>http://localhost:4000/2024/12/07/daily-4</id><content type="html" xml:base="http://localhost:4000/2024/12/07/daily-4.html"><![CDATA[<h2>Laying the Groundwork</h2>

<p>Today, I spent some time working on an API for managing music ratings, inspired by platforms like Rate Your Music. The plan is to design it with OpenAPI in a design-first approach and build it in Go using its standard library. The API will handle CRUD operations for albums and user ratings, calculate averages (both simple and weighted), and provide metadata like the total number of ratings. Drafting the OpenAPI spec was trickier than I expected—figuring out how to clearly define endpoints for weighted averages without overcomplicating things took some effort.</p>

<p>Here’s a link to it:
<a href="https://app.swaggerhub.com/apis/EVANWEVER_1/test/1.0.0">https://app.swaggerhub.com/apis/EVANWEVER_1/test/1.0.0</a></p>

<h2>What’s Next?</h2>

<p>Next up, I’ll refine the OpenAPI spec. I want to make sure the API is not just functional but easy to understand and maintain. Writing handlers for CRUD operations and making sure the endpoints align with the spec has been satisfying, even if the process has been slower than I’d hoped. Overall, it’s been a mix of solving problems and learning more about Go’s way of doing things.</p>

<p>Tomorrow, I’m going to relax and also look at some game modding. If I end up working on that, there’ll be a post for it.</p>]]></content><author><name></name></author><summary type="html"><![CDATA[Laying the Groundwork]]></summary></entry><entry><title type="html">Status Update #3: 12/4-5</title><link href="http://localhost:4000/2024/12/06/daily-3.html" rel="alternate" type="text/html" title="Status Update #3: 12/4-5" /><published>2024-12-06T01:21:02-05:00</published><updated>2024-12-06T01:21:02-05:00</updated><id>http://localhost:4000/2024/12/06/daily-3</id><content type="html" xml:base="http://localhost:4000/2024/12/06/daily-3.html"><![CDATA[<p>Today, I spent some time learning how to set up a simple API, connect it to a database, and manage everything in a development-friendly environment.</p>

<h2>Switching from Node.js to Go</h2>
<p>I started with a tutorial that used Node.js to build an API connected to MongoDB. While it worked, I ran into some issues on Windows that slowed me down. To fix this, I switched to Windows Subsystem for Linux (WSL), which gave me a smoother experience. While making this change, I also decided to try Go instead of Node.js, both to see if its simplified workflow is useful to me and also because it may be relevant to prospects that I have lined up in the future. Rather than installing MongoDB directly on WSL, I decided to run it in a Docker container. This kept things clean and isolated, and it gave me a chance to work with Docker, something I’ve wanted to practice more.</p>

<h2>Building the API in Go</h2>
<p>Setting up the API in Go was a great learning experience. Since this was my first time using Go for a real project, I spent time learning how to define structs, handle JSON, and connect to MongoDB. There were some challenges with mapping MongoDB documents to Go structs, but after some trial and error, I got the basic CRUD operations working.</p>

<p>By the end of the day, I had a simple API that could interact with the MongoDB database through Docker, running entirely in WSL.</p>

<h2>Next Steps: OpenAPI Design</h2>
<p>Now that I have the basic setup working, my next step is to create an OpenAPI specification and align the API with it. Tomorrow, I’ll work on the design-first approach, which should make the API easier to document and more consistent.</p>]]></content><author><name></name></author><summary type="html"><![CDATA[Today, I spent some time learning how to set up a simple API, connect it to a database, and manage everything in a development-friendly environment.]]></summary></entry><entry><title type="html">Status Update #2: 12/3</title><link href="http://localhost:4000/2024/12/03/daily-2.html" rel="alternate" type="text/html" title="Status Update #2: 12/3" /><published>2024-12-03T11:09:02-05:00</published><updated>2024-12-03T11:09:02-05:00</updated><id>http://localhost:4000/2024/12/03/daily-2</id><content type="html" xml:base="http://localhost:4000/2024/12/03/daily-2.html"><![CDATA[<p>I started a course on using the OpenAPI specification today. The first module was about understanding the high-level structure of an OpenAPI document, to give me an idea of the need for API documentation and the general function of APIs before actually building anything. I learned how the spec organizes information about an API: details like the available endpoints, parameters, request and response formats, and even how to define basic security features using HTTP.</p>

<p>An important part was learning how to format query parameters, headers, and body content in a way that someone reading the spec could actually understand what’s going on. The course also covered how to define responses with proper status codes and schemes. It also went over the basics of different security solutions available in the spec.</p>

<p>To make it stick, I followed along with the examples in SwaggerHub by creating my own mock API. So far, it’s a very basic to-do list app with only one endpoint for showing tasks but I plan to develop it as I continue the course. I went with a basic HTTP protocol for the API.</p>

<p><img src="/assets/images/blog2_asset1.png" alt="Example API image" /></p>

<p>While I was working through the course, I got sidetracked by a linked page about REST architecture. I’ve heard of REST before but hadn’t really gone deep on its principles until now. The page broke down how REST builds on a series of constraints, like the client-server model, stateless communication caching and layered models. Taking from some of these constraints, the stateless nature of REST ensures each request is independent which improves scalability. REST also treats everything as a resource — documents, services, or even concepts — and identifies them through resource identifiers like URLs. These resources are interacted with using representations (like HTML or JSON), which carry both data and metadata. Reading about it helped bridge my understanding of API design and broader system architecture.</p>

<p>Switching gears, I’ve also been studying for the Civic Literacy Exam, which is the last thing I need to cross off to finish my bachelor’s degree. I decided to cold-take a few practice exams today just to see where I’m at and need to study Supreme Court cases in particular. I was aware of basic political concepts and history in particular since I’m halfway through a long series of lectures on US history through Great Courses, but when it comes to contemporary history and landmark cases, I realized I need to brush up.</p>]]></content><author><name></name></author><summary type="html"><![CDATA[I started a course on using the OpenAPI specification today. The first module was about understanding the high-level structure of an OpenAPI document, to give me an idea of the need for API documentation and the general function of APIs before actually building anything. I learned how the spec organizes information about an API: details like the available endpoints, parameters, request and response formats, and even how to define basic security features using HTTP.]]></summary></entry><entry><title type="html">Status Update #1: 11/11-11/14</title><link href="http://localhost:4000/2024/11/14/daily-1.html" rel="alternate" type="text/html" title="Status Update #1: 11/11-11/14" /><published>2024-11-14T11:09:02-05:00</published><updated>2024-11-14T11:09:02-05:00</updated><id>http://localhost:4000/2024/11/14/daily-1</id><content type="html" xml:base="http://localhost:4000/2024/11/14/daily-1.html"><![CDATA[<p><img src="/assets/images/blog1_asset1.png" alt="This has a tooltip!" /></p>

<p>Yesterday, I set up this blog using GitHub Pages and Jekyll. Jekyll is a convenient platform for hosting static content since it pairs well with Pages, working ‘out of the box’ by not needing a custom workflow set up to build after changes are committed. It will be nice to centralize just about everything I’m doing in one place. Consider this half devblog and half personal blog for the time being. I’m considering cname’ing this to something other than [https://eawever.github.io/], although I may wait until this site has some more content since this isn’t costing me anything so far. Getting everything up and running required navigating some stuff that I haven’t done in awhile: version control with Git, package installations in Ubuntu, and managing files in a WSL (Windows Subsystem for Linux) terminal.</p>

<h2>Amazon Rekognition</h2>

<p>In parallel, I worked on an interesting lab for my machine learning class, where I explored Amazon Rekognition. This AWS service specializes in image and video analysis, and my task involved creating a custom collection for face recognition. The process began with setting up the SageMaker environment to train the model and create a custom Rekognition collection. Once the collection was ready, I uploaded images and added them to the collection to teach the model about specific individuals. Finally, I tested the system by feeding it a new image and used Rekognition to detect and identify known faces.</p>

<h2>Building an ETL Pipeline with Step Functions</h2>

<p>Another significant project this week was for my Cloud Data Analytics class, where I built an ETL pipeline using AWS Step Functions. The goal was to automate the extraction, transformation, and loading of a large dataset. This pipeline integrated several AWS services: Amazon S3 for data storage, AWS Glue for data transformation and cataloging, and Amazon Athena for querying the processed data.</p>

<p><img src="/assets/images/step-functions-example.png" alt="This also has a tooltip!" /></p>

<p>AWS Step Functions played a central role in orchestrating the workflow. I configured the state machine to manage each stage, ensuring that data moved seamlessly from S3 to Glue, where it was cleaned and prepared, and then to Athena for analysis. It helped a bit in getting my head around serverless architecture for building data pipelines in AWS.</p>

<h2>Extra bits</h2>

<p>On a different note, I’ve also been working on improving my Japanese. Up until now, about 90% of my learning has been through Duolingo, which has been great for building a foundational vocabulary and getting a feel for basic grammar structures. However, I’m realizing that to truly progress, I need to move beyond just gamified lessons.</p>

<p><img src="/assets/images/blog1_asset3.jpeg" alt="This additionally has a tooltip." /></p>

<p>Lately, I’ve been watching Japanese videos with Japanese subtitles to get accustomed to native speech patterns and improve my reading skills simultaneously. It’s been challenging but incredibly rewarding when I catch words that I’ve learned. I’m waiting until I learn a little more basic Kanji in my Duolingo lessons to start Anki, but should work that into my routine very soon.</p>]]></content><author><name></name></author><summary type="html"><![CDATA[]]></summary></entry></feed>